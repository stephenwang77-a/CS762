{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs:  \n",
    "\n",
    "Arrythmia:  \n",
    "Decision Tree Classifier: 65.62% accuracy with a standard deviation of 5.04%  \n",
    "Random Forest Classifier: 71.13% accuracy with a standard deviation of 4.51%  \n",
    "Ada Boost Classifier: 61.76% accuracy with a standard deviation of 4.91%  \n",
    "Bagging Classifier: 75.16% accuracy with a standard deviation of 4.30%  \n",
    "Extra Trees Classifier: 69.85% accuracy with a standard deviation of 4.47%  \n",
    "  \n",
    "Caesarian:  \n",
    "Decision Tree Classifier: 52.68% accuracy with a standard deviation of 11.32%  \n",
    "Random Forest Classifier: 54.85% accuracy with a standard deviation of 12.75%  \n",
    "Ada Boost Classifier: 60.97% accuracy with a standard deviation of 11.22%  \n",
    "Bagging Classifier: 54.43% accuracy with a standard deviation of 11.48%  \n",
    "Extra Trees Classifier: 53.50% accuracy with a standard deviation of 10.96%  \n",
    "  \n",
    "Phishing:    \n",
    "Decision Tree Classifier: 96.29% accuracy with a standard deviation of 0.38%  \n",
    "Random Forest Classifier: 96.99% accuracy with a standard deviation of 0.34%  \n",
    "Ada Boost Classifier: 92.90% accuracy with a standard deviation of 0.53%  \n",
    "Bagging Classifier: 96.79% accuracy with a standard deviation of 0.35%  \n",
    "Extra Trees Classifier: 97.15% accuracy with a standard deviation of 0.34%   \n",
    "  \n",
    "Wine:    \n",
    "Decision Tree Classifier: 90.70% accuracy with a standard deviation of 4.54%  \n",
    "Random Forest Classifier: 96.96% accuracy with a standard deviation of 2.86%  \n",
    "Ada Boost Classifier: 88.63% accuracy with a standard deviation of 8.29%  \n",
    "Bagging Classifier: 95.38% accuracy with a standard deviation of 3.56%  \n",
    "Extra Trees Classifier: 96.74% accuracy with a standard deviation of 2.82%  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimental design:\n",
    "\n",
    "For this study, we compare classification ensemble algorithms with three different datasets and infer why a particular algorithm would achieve the highest accuracy for a specific dataset. When building the design of this experiment, we assume default settings across all parameters.  \n",
    "\n",
    "To increase the validity of our results, we iteratively fitted the ensemble models with new randomized training data (80% subset of original dataset) and cross-validated the model using repeated k-folds (folds=5, repeat=10). We repeat this five times in total, each iteration with a different seed, before averaging the outcomes to reduce noisy estimation of model accuracy from issues such as fluke training/test data split.\n",
    "\n",
    "We used the repeated k-fold cross-validation to estimate the performance of the ensembles since this method reports the mean accuracy result across all folds from ten repetitions as per the default settings. This reduces the error in the mean estimate of the model performance compared to a single cross-validation without repeats.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results:\n",
    "\n",
    "The bagging ensemble algorithm performed the best on the arrhythmia dataset with an accuracy of 75.16% and the minimal standard deviation of 4.30%. Bagging generates multiple bootstrap samples from the original dataset (with replacement) where each sample is independently used to fit their prediction model and then combined into an aggregated prediction. This decreases the variance of the prediction since the bagging approach creates subsets which often overlap to model the data. As a result, we suspect bagging to be the best ensemble for the arrhythmia dataset because the method handles higher dimensionality data compared to the other ensembles. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s contrast this to AdaBoost which performed the best on the caesarian dataset which only contained five features to explain the binary response. AdaBoost builds on decision stumps (one split per tree) and the boosting algorithm sequentially trains the model and adjusts an observation's weight depending on if the prediction was correct or incorrect, until eventually a predetermined number of trees are trained or observations are perfectly predicted. As a result, AdaBoost performs less well on the other datasets with more features as it is more sensitive to outliers and noise but performs the best on the caesarian dataset where performance is increased on datasets with small number of features that have strong predictive powers. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, the extra trees ensemble performed the best on the website phishing dataset but if we take a closer look at the random forest figures, there's only a 0.16% difference between the two. \n",
    "\n",
    "There are two main differences between extra trees and random trees:  \n",
    "(1) Extra trees sample without replacement  \n",
    "(2) And split points are randomized (not optimized like in random trees)    \n",
    "\n",
    "The extra randomness may outperform random forests since mistakes are less correlated to each other and more concerned with the random split. Once the split points are selected, the algorithm chooses the best one between the subsets of features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our wine dataset (imported from the sklearn module), random forest performed the best which is also the ensemble that consistently performed strongly across all datasets. Random forests is a robust ensemble that is well equipped to deal with noise and outliers by discarding those values, it handles collinearity well because one of the features will use up the predictive power of the other feature, and it can manage large datasets with high dimensionality by ignoring features that do not provide the optimal split. This experiment proves random forests is a great all-rounder for supervised learning and can be used with multiple types of datasets to produce a relatively powerful predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from autorank import autorank, create_report, plot_stats\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrhythmia: \n",
    "\n",
    "The \"J\" column had the majority of ? values so we assumed it wouldn't be useful in explaining arrhythmia in our models. After dropping the \"J\" column, we replaced the ? values with NaN and then dropped the rows which contained NaN. This trimmed our dataset down to 420 rows and 279 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 10\n",
    "RANDOM_STATE = 1234 \n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "arrhythmia = pd.read_csv(\"arrhythmia.csv\")\n",
    "arrhythmia.columns = arrhythmia.columns.str.strip()\n",
    "\n",
    "# Cleaning ? values \n",
    "arrhythmia = arrhythmia.drop([\"J\"], axis=1)\n",
    "arrhythmia = arrhythmia.replace(\"?\", np.nan).dropna()\n",
    "\n",
    "arrhythmia_sd = []\n",
    "arrhythmia_means = []\n",
    "\n",
    "caesarian = pd.read_csv(\"caesarian.csv\")\n",
    "caesarian.columns = caesarian.columns.str.strip()\n",
    "caesarian_sd = []\n",
    "caesarian_means = []\n",
    "\n",
    "phishing = pd.read_csv(\"website-phishing.csv\")\n",
    "phishing.columns = phishing.columns.str.strip()\n",
    "phishing_sd = []\n",
    "phishing_means = []\n",
    "\n",
    "wine = load_wine()\n",
    "wine_sd = []\n",
    "wine_means = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6562380952380952, 0.7112857142857142, 0.6175714285714285, 0.7515714285714286, 0.6985238095238093]\n",
      "   DecisionTree  RandomForest  AdaBoost   Bagging  ExtraTrees\n",
      "0      0.659524      0.708333  0.622143  0.753333    0.690714\n",
      "1      0.646905      0.720000  0.619286  0.748810    0.696429\n",
      "2      0.661667      0.704286  0.616667  0.748095    0.704524\n",
      "3      0.663571      0.705476  0.613571  0.753333    0.699524\n",
      "4      0.649524      0.718333  0.616190  0.754286    0.701429\n",
      "RankResult(rankdf=\n",
      "              meanrank      mean       std  ci_lower  ci_upper effect_size  \\\n",
      "Bagging            1.0  0.751571  0.002885   0.74626  0.756883           0   \n",
      "RandomForest       2.2  0.711286  0.007367  0.705974  0.716597      7.2013   \n",
      "ExtraTrees         2.8  0.698524  0.005263  0.693213  0.703835     12.4989   \n",
      "DecisionTree       4.0  0.656238  0.007521  0.650927  0.661549     16.7379   \n",
      "AdaBoost           5.0  0.617571  0.003262   0.61226  0.622883     43.5182   \n",
      "\n",
      "               magnitude  \n",
      "Bagging       negligible  \n",
      "RandomForest       large  \n",
      "ExtraTrees         large  \n",
      "DecisionTree       large  \n",
      "AdaBoost           large  \n",
      "pvalue=2.81959142822369e-15\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.2558911442756653, 0.18069496750831604, 0.9099575877189636, 0.10539042204618454, 0.8956444263458252]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.27256428893360884\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=5\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n",
      "The statistical analysis was conducted for 5 populations with 5 paired samples.\n",
      "The family-wise significance level of the tests is alpha=0.050.\n",
      "We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.105). Therefore, we assume that all populations are normal.\n",
      "We applied Bartlett's test for homogeneity and failed to reject the null hypothesis (p=0.273) that the data is homoscedastic. Thus, we assume that our data is homoscedastic.\n",
      "Because we have more than two populations and all populations are normal and homoscedastic, we use repeated measures ANOVA as omnibus test to determine if there are any significant differences between the mean values of the populations. If the results of the ANOVA test are significant, we use the post-hoc Tukey HSD test to infer which differences are significant. We report the mean value (M) and the standard deviation (SD) for each population. Populations are significantly different if their confidence intervals are not overlapping.\n",
      "We reject the null hypothesis (p=0.000) of the repeated measures ANOVA that there is a difference between the mean values of the populations Bagging (M=0.752+-0.005, SD=0.003), RandomForest (M=0.711+-0.005, SD=0.007), ExtraTrees (M=0.699+-0.005, SD=0.005), DecisionTree (M=0.656+-0.005, SD=0.008), and AdaBoost (M=0.618+-0.005, SD=0.003). Therefore, we assume that there is a statistically significant difference between the mean values of the populations.\n",
      "Based on post-hoc Tukey HSD test, we assume that all differences between the populations are significant.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenwang/opt/anaconda3/lib/python3.8/site-packages/statsmodels/sandbox/stats/multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    }
   ],
   "source": [
    "# Splitting response and explanatory variables \n",
    "\n",
    "X=arrhythmia.drop([\"class\"], axis=1)\n",
    "y=arrhythmia[\"class\"]\n",
    "\n",
    "random_seed = 1234\n",
    "decision_tree_temp = []\n",
    "random_forest_temp = []\n",
    "ada_boost_temp = []\n",
    "bagging_model_temp = []\n",
    "extra_trees_temp = []\n",
    "\n",
    "decision_tree_sd_temp = []\n",
    "random_forest_sd_temp = []\n",
    "ada_boost_sd_temp = []\n",
    "bagging_model_sd_temp = []\n",
    "extra_trees_sd_temp = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    multi = RepeatedKFold(n_splits = 5, random_state = random_seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    # Training the Decision Tree classifier\n",
    "    \n",
    "    decision_tree_model = DecisionTreeClassifier(random_state=random_seed)\n",
    "    decision_tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training the Random Forest classifier\n",
    "\n",
    "    random_forest_model = RandomForestClassifier(n_estimators=10, random_state=random_seed)\n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Ada Boost classifier \n",
    "\n",
    "    ada_boost_model = AdaBoostClassifier(n_estimators=10, random_state=random_seed)\n",
    "    ada_boost_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Bagging Model classifier \n",
    "\n",
    "    bagging_model = BaggingClassifier(n_estimators=10, random_state=random_seed)\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Extra Trees classifier\n",
    "\n",
    "    extra_trees_model = ExtraTreesClassifier(n_estimators=10, random_state=random_seed).fit(X_train, y_train)\n",
    "    \n",
    "    decision_tree_score = cross_val_score(decision_tree_model, X, y, cv=multi)\n",
    "    random_forest_model_score = cross_val_score(random_forest_model, X, y, cv=multi)\n",
    "    ada_boost_model_score = cross_val_score(ada_boost_model, X, y, cv=multi)\n",
    "    bagging_model_score = cross_val_score(bagging_model, X, y, cv=multi)\n",
    "    extra_trees_score = cross_val_score(extra_trees_model, X, y, cv=multi)\n",
    "    \n",
    "    # Append the standard deviation scores to respective temporary lists\n",
    "    \n",
    "    decision_tree_sd_temp.append(decision_tree_score.std())\n",
    "    random_forest_sd_temp.append(random_forest_model_score.std())\n",
    "    ada_boost_sd_temp.append(ada_boost_model_score.std())\n",
    "    bagging_model_sd_temp.append(bagging_model_score.std())\n",
    "    extra_trees_sd_temp.append(extra_trees_score.std())\n",
    "\n",
    "    \n",
    "    # Append the mean of CV scores to respective temporary lists \n",
    "    \n",
    "    decision_tree_temp.append(decision_tree_score.mean())\n",
    "    random_forest_temp.append(random_forest_model_score.mean())\n",
    "    ada_boost_temp.append(ada_boost_model_score.mean())\n",
    "    bagging_model_temp.append(bagging_model_score.mean())\n",
    "    extra_trees_temp.append(extra_trees_score.mean())\n",
    "\n",
    "    random_seed += 999\n",
    "\n",
    "# Append the average of all means across CV runs \n",
    "\n",
    "arrhythmia_means.append(np.mean(decision_tree_temp))\n",
    "arrhythmia_means.append(np.mean(random_forest_temp))\n",
    "arrhythmia_means.append(np.mean(ada_boost_temp))\n",
    "arrhythmia_means.append(np.mean(bagging_model_temp))\n",
    "arrhythmia_means.append(np.mean(extra_trees_temp))\n",
    "\n",
    "arrhythmia_sd.append(np.mean(decision_tree_sd_temp))\n",
    "arrhythmia_sd.append(np.mean(random_forest_sd_temp))\n",
    "arrhythmia_sd.append(np.mean(ada_boost_sd_temp))\n",
    "arrhythmia_sd.append(np.mean(bagging_model_sd_temp))\n",
    "arrhythmia_sd.append(np.mean(extra_trees_sd_temp))\n",
    "\n",
    "print(arrhythmia_means)\n",
    "\n",
    "x = {'DecisionTree':decision_tree_temp,\n",
    "     'RandomForest':random_forest_temp,\n",
    "     'AdaBoost':ada_boost_temp,\n",
    "     'Bagging': bagging_model_temp,\n",
    "     'ExtraTrees': extra_trees_temp,\n",
    "    }\n",
    "df = pd.DataFrame (x, columns = ['DecisionTree','RandomForest','AdaBoost','Bagging','ExtraTrees'])\n",
    "print(df)\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RankResult(rankdf=\n",
      "              meanrank     mean       std  ci_lower  ci_upper effect_size  \\\n",
      "AdaBoost           1.0  0.60975  0.007469  0.597623  0.621877           0   \n",
      "RandomForest       2.5  0.54850  0.013619  0.536373  0.560627     5.57683   \n",
      "Bagging            3.1  0.54425  0.012766  0.532123  0.556377       6.263   \n",
      "ExtraTrees         3.8  0.53500  0.011924  0.522873  0.547127     7.51325   \n",
      "DecisionTree       4.6  0.52675  0.016574  0.514623  0.538877     6.45696   \n",
      "\n",
      "               magnitude  \n",
      "AdaBoost      negligible  \n",
      "RandomForest       large  \n",
      "Bagging            large  \n",
      "ExtraTrees         large  \n",
      "DecisionTree       large  \n",
      "pvalue=4.475291790504448e-08\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.5007793307304382, 0.2373344749212265, 0.9443285465240479, 0.07801619917154312, 0.19198185205459595]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.7054695098014168\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=5\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n",
      "The statistical analysis was conducted for 5 populations with 5 paired samples.\n",
      "The family-wise significance level of the tests is alpha=0.050.\n",
      "We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.078). Therefore, we assume that all populations are normal.\n",
      "We applied Bartlett's test for homogeneity and failed to reject the null hypothesis (p=0.705) that the data is homoscedastic. Thus, we assume that our data is homoscedastic.\n",
      "Because we have more than two populations and all populations are normal and homoscedastic, we use repeated measures ANOVA as omnibus test to determine if there are any significant differences between the mean values of the populations. If the results of the ANOVA test are significant, we use the post-hoc Tukey HSD test to infer which differences are significant. We report the mean value (M) and the standard deviation (SD) for each population. Populations are significantly different if their confidence intervals are not overlapping.\n",
      "We reject the null hypothesis (p=0.000) of the repeated measures ANOVA that there is a difference between the mean values of the populations AdaBoost (M=0.610+-0.012, SD=0.007), RandomForest (M=0.548+-0.012, SD=0.014), Bagging (M=0.544+-0.012, SD=0.013), ExtraTrees (M=0.535+-0.012, SD=0.012), and DecisionTree (M=0.527+-0.012, SD=0.017). Therefore, we assume that there is a statistically significant difference between the mean values of the populations.\n",
      "Based post-hoc Tukey HSD test, we assume that there are no significant differences within the following groups: RandomForest and Bagging; Bagging and ExtraTrees; ExtraTrees and DecisionTree. All other differences are significant.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenwang/opt/anaconda3/lib/python3.8/site-packages/statsmodels/sandbox/stats/multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    }
   ],
   "source": [
    "random_seed = 1234\n",
    "decision_tree_temp = []\n",
    "random_forest_temp = []\n",
    "ada_boost_temp = []\n",
    "bagging_model_temp = []\n",
    "extra_trees_temp = []\n",
    "\n",
    "decision_tree_sd_temp = []\n",
    "random_forest_sd_temp = []\n",
    "ada_boost_sd_temp = []\n",
    "bagging_model_sd_temp = []\n",
    "extra_trees_sd_temp = []\n",
    "\n",
    "X=caesarian.drop([\"class\"], axis=1)\n",
    "y=caesarian[\"class\"]\n",
    "\n",
    "for i in range(5):\n",
    "    multi = RepeatedKFold(n_splits = 5, random_state = random_seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    # Training the Decision Tree classifier\n",
    "    \n",
    "    decision_tree_model = DecisionTreeClassifier(random_state=random_seed)\n",
    "    decision_tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training the Random Forest classifier\n",
    "\n",
    "    random_forest_model = RandomForestClassifier(n_estimators=10, random_state=random_seed)\n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Ada Boost classifier \n",
    "\n",
    "    ada_boost_model = AdaBoostClassifier(n_estimators=10, random_state=random_seed)\n",
    "    ada_boost_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Bagging Model classifier \n",
    "\n",
    "    bagging_model = BaggingClassifier(n_estimators=10, random_state=random_seed)\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Extra Trees classifier\n",
    "\n",
    "    extra_trees_model = ExtraTreesClassifier(n_estimators=10, random_state=random_seed).fit(X_train, y_train)\n",
    "    \n",
    "    decision_tree_score = cross_val_score(decision_tree_model, X, y, cv=multi)\n",
    "    random_forest_model_score = cross_val_score(random_forest_model, X, y, cv=multi)\n",
    "    ada_boost_model_score = cross_val_score(ada_boost_model, X, y, cv=multi)\n",
    "    bagging_model_score = cross_val_score(bagging_model, X, y, cv=multi)\n",
    "    extra_trees_score = cross_val_score(extra_trees_model, X, y, cv=multi)\n",
    "    \n",
    "    # Append the standard deviation scores to respective temporary lists\n",
    "    \n",
    "    decision_tree_sd_temp.append(decision_tree_score.std())\n",
    "    random_forest_sd_temp.append(random_forest_model_score.std())\n",
    "    ada_boost_sd_temp.append(ada_boost_model_score.std())\n",
    "    bagging_model_sd_temp.append(bagging_model_score.std())\n",
    "    extra_trees_sd_temp.append(extra_trees_score.std())\n",
    "\n",
    "    \n",
    "    decision_tree_temp.append(decision_tree_score.mean())\n",
    "    random_forest_temp.append(random_forest_model_score.mean())\n",
    "    ada_boost_temp.append(ada_boost_model_score.mean())\n",
    "    bagging_model_temp.append(bagging_model_score.mean())\n",
    "    extra_trees_temp.append(extra_trees_score.mean())\n",
    "\n",
    "    random_seed += 999\n",
    "\n",
    "\n",
    "caesarian_means.append(np.mean(decision_tree_temp))\n",
    "caesarian_means.append(np.mean(random_forest_temp))\n",
    "caesarian_means.append(np.mean(ada_boost_temp))\n",
    "caesarian_means.append(np.mean(bagging_model_temp))\n",
    "caesarian_means.append(np.mean(extra_trees_temp))\n",
    "\n",
    "caesarian_sd.append(np.mean(decision_tree_sd_temp))\n",
    "caesarian_sd.append(np.mean(random_forest_sd_temp))\n",
    "caesarian_sd.append(np.mean(ada_boost_sd_temp))\n",
    "caesarian_sd.append(np.mean(bagging_model_sd_temp))\n",
    "caesarian_sd.append(np.mean(extra_trees_sd_temp))\n",
    "\n",
    "x = {'DecisionTree':decision_tree_temp,\n",
    "     'RandomForest':random_forest_temp,\n",
    "     'AdaBoost':ada_boost_temp,\n",
    "     'Bagging': bagging_model_temp,\n",
    "     'ExtraTrees': extra_trees_temp,\n",
    "    }\n",
    "df = pd.DataFrame (x, columns = ['DecisionTree','RandomForest','AdaBoost','Bagging','ExtraTrees'])\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DecisionTree  RandomForest  AdaBoost   Bagging  ExtraTrees\n",
      "0      0.963220      0.970095  0.929037  0.968204    0.971588\n",
      "1      0.962994      0.970185  0.929073  0.968295    0.971253\n",
      "2      0.963238      0.969869  0.928820  0.967716    0.972013\n",
      "3      0.962795      0.969462  0.929236  0.967644    0.971325\n",
      "4      0.962388      0.969787  0.928937  0.967734    0.971398\n",
      "RankResult(rankdf=\n",
      "              meanrank      mean       std  ci_lower  ci_upper effect_size  \\\n",
      "ExtraTrees         1.0  0.971515  0.000305  0.971242  0.971788           0   \n",
      "RandomForest       2.0  0.969880  0.000284  0.969607  0.970152     5.55035   \n",
      "Bagging            3.0  0.967919  0.000306  0.967646  0.968191     11.7816   \n",
      "DecisionTree       4.0  0.962927  0.000352  0.962654    0.9632     26.0904   \n",
      "AdaBoost           5.0  0.929020  0.000155  0.928748  0.929293     175.675   \n",
      "\n",
      "               magnitude  \n",
      "ExtraTrees    negligible  \n",
      "RandomForest       large  \n",
      "Bagging            large  \n",
      "DecisionTree       large  \n",
      "AdaBoost           large  \n",
      "pvalue=6.703156753230692e-30\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.41116565465927124, 0.7658283114433289, 0.9802923798561096, 0.09925013780593872, 0.2646442949771881]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.6814026599916041\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=5\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n",
      "The statistical analysis was conducted for 5 populations with 5 paired samples.\n",
      "The family-wise significance level of the tests is alpha=0.050.\n",
      "We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.099). Therefore, we assume that all populations are normal.\n",
      "We applied Bartlett's test for homogeneity and failed to reject the null hypothesis (p=0.681) that the data is homoscedastic. Thus, we assume that our data is homoscedastic.\n",
      "Because we have more than two populations and all populations are normal and homoscedastic, we use repeated measures ANOVA as omnibus test to determine if there are any significant differences between the mean values of the populations. If the results of the ANOVA test are significant, we use the post-hoc Tukey HSD test to infer which differences are significant. We report the mean value (M) and the standard deviation (SD) for each population. Populations are significantly different if their confidence intervals are not overlapping.\n",
      "We reject the null hypothesis (p=0.000) of the repeated measures ANOVA that there is a difference between the mean values of the populations ExtraTrees (M=0.972+-0.000, SD=0.000), RandomForest (M=0.970+-0.000, SD=0.000), Bagging (M=0.968+-0.000, SD=0.000), DecisionTree (M=0.963+-0.000, SD=0.000), and AdaBoost (M=0.929+-0.000, SD=0.000). Therefore, we assume that there is a statistically significant difference between the mean values of the populations.\n",
      "Based on post-hoc Tukey HSD test, we assume that all differences between the populations are significant.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenwang/opt/anaconda3/lib/python3.8/site-packages/statsmodels/sandbox/stats/multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    }
   ],
   "source": [
    "random_seed = 1234\n",
    "decision_tree_temp = []\n",
    "random_forest_temp = []\n",
    "ada_boost_temp = []\n",
    "bagging_model_temp = []\n",
    "extra_trees_temp = []\n",
    "\n",
    "decision_tree_sd_temp = []\n",
    "random_forest_sd_temp = []\n",
    "ada_boost_sd_temp = []\n",
    "bagging_model_sd_temp = []\n",
    "extra_trees_sd_temp = []\n",
    "\n",
    "X=phishing.drop([\"Class\"], axis=1)\n",
    "y=phishing[\"Class\"]\n",
    "\n",
    "for i in range(5):\n",
    "    multi = RepeatedKFold(n_splits=5, random_state=random_seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    # Training the Decision Tree classifier\n",
    "    \n",
    "    decision_tree_model = DecisionTreeClassifier(random_state=random_seed)\n",
    "    decision_tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training the Random Forest classifier\n",
    "\n",
    "    random_forest_model = RandomForestClassifier(n_estimators=10, random_state=random_seed)\n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Ada Boost classifier \n",
    "\n",
    "    ada_boost_model = AdaBoostClassifier(n_estimators=10, random_state=random_seed)\n",
    "    ada_boost_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Bagging Model classifier \n",
    "\n",
    "    bagging_model = BaggingClassifier(n_estimators=10, random_state=random_seed)\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Extra Trees classifier\n",
    "\n",
    "    extra_trees_model = ExtraTreesClassifier(n_estimators=10, random_state=random_seed).fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    decision_tree_score = cross_val_score(decision_tree_model, X, y, cv=multi)\n",
    "    random_forest_model_score = cross_val_score(random_forest_model, X, y, cv=multi)\n",
    "    ada_boost_model_score = cross_val_score(ada_boost_model, X, y, cv=multi)\n",
    "    bagging_model_score = cross_val_score(bagging_model, X, y, cv=multi)\n",
    "    extra_trees_score = cross_val_score(extra_trees_model, X, y, cv=multi)\n",
    "    \n",
    "    \n",
    "    # Append the standard deviation scores to respective temporary lists\n",
    "    \n",
    "    decision_tree_sd_temp.append(decision_tree_score.std())\n",
    "    random_forest_sd_temp.append(random_forest_model_score.std())\n",
    "    ada_boost_sd_temp.append(ada_boost_model_score.std())\n",
    "    bagging_model_sd_temp.append(bagging_model_score.std())\n",
    "    extra_trees_sd_temp.append(extra_trees_score.std())\n",
    "\n",
    "    decision_tree_temp.append(decision_tree_score.mean())\n",
    "    random_forest_temp.append(random_forest_model_score.mean())\n",
    "    ada_boost_temp.append(ada_boost_model_score.mean())\n",
    "    bagging_model_temp.append(bagging_model_score.mean())\n",
    "    extra_trees_temp.append(extra_trees_score.mean())\n",
    "\n",
    "    random_seed += 999\n",
    "\n",
    "phishing_means.append(np.mean(decision_tree_temp))\n",
    "phishing_means.append(np.mean(random_forest_temp))\n",
    "phishing_means.append(np.mean(ada_boost_temp))\n",
    "phishing_means.append(np.mean(bagging_model_temp))\n",
    "phishing_means.append(np.mean(extra_trees_temp))\n",
    "\n",
    "phishing_sd.append(np.mean(decision_tree_sd_temp))\n",
    "phishing_sd.append(np.mean(random_forest_sd_temp))\n",
    "phishing_sd.append(np.mean(ada_boost_sd_temp))\n",
    "phishing_sd.append(np.mean(bagging_model_sd_temp))\n",
    "phishing_sd.append(np.mean(extra_trees_sd_temp))\n",
    "\n",
    "\n",
    "x = {'DecisionTree':decision_tree_temp,\n",
    "     'RandomForest':random_forest_temp,\n",
    "     'AdaBoost':ada_boost_temp,\n",
    "     'Bagging': bagging_model_temp,\n",
    "     'ExtraTrees': extra_trees_temp,\n",
    "    }\n",
    "df = pd.DataFrame (x, columns = ['DecisionTree','RandomForest','AdaBoost','Bagging','ExtraTrees'])\n",
    "print(df)\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   DecisionTree  RandomForest  AdaBoost   Bagging  ExtraTrees\n",
      "0      0.906857      0.970651  0.887222  0.952286    0.967476\n",
      "1      0.907429      0.966302  0.891016  0.952794    0.967429\n",
      "2      0.901111      0.968587  0.889127  0.954476    0.961746\n",
      "3      0.905667      0.970270  0.881873  0.954413    0.971365\n",
      "4      0.913968      0.972397  0.882476  0.954905    0.969000\n",
      "RankResult(rankdf=\n",
      "              meanrank      mean       std  ci_lower  ci_upper effect_size  \\\n",
      "RandomForest       1.4  0.969641  0.002306   0.96645  0.972832           0   \n",
      "ExtraTrees         1.6  0.967403  0.003545  0.964212  0.970594    0.748492   \n",
      "Bagging            3.0  0.953775  0.001157  0.950584  0.956966     8.69694   \n",
      "DecisionTree       4.0  0.907006  0.004616  0.903815  0.910197     17.1676   \n",
      "AdaBoost           5.0  0.886343  0.004040  0.883152  0.889534     25.3229   \n",
      "\n",
      "               magnitude  \n",
      "RandomForest  negligible  \n",
      "ExtraTrees        medium  \n",
      "Bagging            large  \n",
      "DecisionTree       large  \n",
      "AdaBoost           large  \n",
      "pvalue=3.9972847470561774e-17\n",
      "cd=None\n",
      "omnibus=anova\n",
      "posthoc=tukeyhsd\n",
      "all_normal=True\n",
      "pvals_shapiro=[0.6965855956077576, 0.8934932947158813, 0.43133455514907837, 0.23025165498256683, 0.48373469710350037]\n",
      "homoscedastic=True\n",
      "pval_homogeneity=0.15982772296691605\n",
      "homogeneity_test=bartlett\n",
      "alpha=0.05\n",
      "alpha_normality=0.01\n",
      "num_samples=5\n",
      "posterior_matrix=\n",
      "None\n",
      "decision_matrix=\n",
      "None\n",
      "rope=None\n",
      "rope_mode=None\n",
      "effect_size=cohen_d)\n",
      "The statistical analysis was conducted for 5 populations with 5 paired samples.\n",
      "The family-wise significance level of the tests is alpha=0.050.\n",
      "We failed to reject the null hypothesis that the population is normal for all populations (minimal observed p-value=0.230). Therefore, we assume that all populations are normal.\n",
      "We applied Bartlett's test for homogeneity and failed to reject the null hypothesis (p=0.160) that the data is homoscedastic. Thus, we assume that our data is homoscedastic.\n",
      "Because we have more than two populations and all populations are normal and homoscedastic, we use repeated measures ANOVA as omnibus test to determine if there are any significant differences between the mean values of the populations. If the results of the ANOVA test are significant, we use the post-hoc Tukey HSD test to infer which differences are significant. We report the mean value (M) and the standard deviation (SD) for each population. Populations are significantly different if their confidence intervals are not overlapping.\n",
      "We reject the null hypothesis (p=0.000) of the repeated measures ANOVA that there is a difference between the mean values of the populations RandomForest (M=0.970+-0.003, SD=0.002), ExtraTrees (M=0.967+-0.003, SD=0.004), Bagging (M=0.954+-0.003, SD=0.001), DecisionTree (M=0.907+-0.003, SD=0.005), and AdaBoost (M=0.886+-0.003, SD=0.004). Therefore, we assume that there is a statistically significant difference between the mean values of the populations.\n",
      "Based post-hoc Tukey HSD test, we assume that there are no significant differences within the following groups: RandomForest and ExtraTrees. All other differences are significant.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/stephenwang/opt/anaconda3/lib/python3.8/site-packages/statsmodels/sandbox/stats/multicomp.py:775: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax1.set_yticklabels(np.insert(self.groupsunique.astype(str), 0, ''))\n"
     ]
    }
   ],
   "source": [
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "random_seed = 1234\n",
    "decision_tree_temp = []\n",
    "random_forest_temp = []\n",
    "ada_boost_temp = []\n",
    "bagging_model_temp = []\n",
    "extra_trees_temp = []\n",
    "\n",
    "decision_tree_sd_temp = []\n",
    "random_forest_sd_temp = []\n",
    "ada_boost_sd_temp = []\n",
    "bagging_model_sd_temp = []\n",
    "extra_trees_sd_temp = []\n",
    "\n",
    "for i in range(5):\n",
    "    multi = RepeatedKFold(n_splits = 5, random_state = random_seed)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "    \n",
    "    # Training the Decision Tree classifier\n",
    "    \n",
    "    decision_tree_model = DecisionTreeClassifier(random_state=random_seed)\n",
    "    decision_tree_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Training the Random Forest classifier\n",
    "\n",
    "    random_forest_model = RandomForestClassifier(n_estimators=10, random_state=random_seed)\n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Ada Boost classifier \n",
    "\n",
    "    ada_boost_model = AdaBoostClassifier(n_estimators=10, random_state=random_seed)\n",
    "    ada_boost_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Bagging Model classifier \n",
    "\n",
    "    bagging_model = BaggingClassifier(n_estimators=10, random_state=random_seed)\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "\n",
    "    # Training Extra Trees classifier\n",
    "\n",
    "    extra_trees_model = ExtraTreesClassifier(n_estimators=10, random_state=random_seed).fit(X_train, y_train)\n",
    "    \n",
    "    decision_tree_score = cross_val_score(decision_tree_model, X, y, cv=multi)\n",
    "    random_forest_model_score = cross_val_score(random_forest_model, X, y, cv=multi)\n",
    "    ada_boost_model_score = cross_val_score(ada_boost_model, X, y, cv=multi)\n",
    "    bagging_model_score = cross_val_score(bagging_model, X, y, cv=multi)\n",
    "    extra_trees_score = cross_val_score(extra_trees_model, X, y, cv=multi)\n",
    "    \n",
    "    # Append the standard deviation scores to respective temporary lists\n",
    "    \n",
    "    decision_tree_sd_temp.append(decision_tree_score.std())\n",
    "    random_forest_sd_temp.append(random_forest_model_score.std())\n",
    "    ada_boost_sd_temp.append(ada_boost_model_score.std())\n",
    "    bagging_model_sd_temp.append(bagging_model_score.std())\n",
    "    extra_trees_sd_temp.append(extra_trees_score.std())\n",
    "\n",
    "    \n",
    "    decision_tree_temp.append(decision_tree_score.mean())\n",
    "    random_forest_temp.append(random_forest_model_score.mean())\n",
    "    ada_boost_temp.append(ada_boost_model_score.mean())\n",
    "    bagging_model_temp.append(bagging_model_score.mean())\n",
    "    extra_trees_temp.append(extra_trees_score.mean())\n",
    "\n",
    "    random_seed += 999\n",
    "\n",
    "wine_means.append(np.mean(decision_tree_temp))\n",
    "wine_means.append(np.mean(random_forest_temp))\n",
    "wine_means.append(np.mean(ada_boost_temp))\n",
    "wine_means.append(np.mean(bagging_model_temp))\n",
    "wine_means.append(np.mean(extra_trees_temp))\n",
    "\n",
    "\n",
    "wine_sd.append(np.mean(decision_tree_sd_temp))\n",
    "wine_sd.append(np.mean(random_forest_sd_temp))\n",
    "wine_sd.append(np.mean(ada_boost_sd_temp))\n",
    "wine_sd.append(np.mean(bagging_model_sd_temp))\n",
    "wine_sd.append(np.mean(extra_trees_sd_temp))\n",
    "\n",
    "x = {'DecisionTree':decision_tree_temp,\n",
    "     'RandomForest':random_forest_temp,\n",
    "     'AdaBoost':ada_boost_temp,\n",
    "     'Bagging': bagging_model_temp,\n",
    "     'ExtraTrees': extra_trees_temp,\n",
    "    }\n",
    "df = pd.DataFrame (x, columns = ['DecisionTree','RandomForest','AdaBoost','Bagging','ExtraTrees'])\n",
    "print(df)\n",
    "result = autorank(df, verbose=False)\n",
    "print(result)\n",
    "create_report(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
